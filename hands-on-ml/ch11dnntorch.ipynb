{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c4123a",
   "metadata": {},
   "source": [
    "Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "Load CIFAR10 just like you loaded the FashionMNIST dataset in Chapter 10, but using torchvision.datasets.CIFAR10 instead of FashionMNIST. The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes.\n",
    "Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the Swish activation function (using nn.SiLU). Since this is a classification task, you will need an output layer with one neuron per class.\n",
    "Using NAdam optimization and early stopping, train the network on the CIFAR10 dataset. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.\n",
    "Now try adding batch-norm and compare the learning curves: is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "Try replacing batch-norm with SELU, and make the necessary adjustments to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
    "Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC dropout.\n",
    "Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3ba1f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import random_split, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa7e2ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CIFAR10(root='./data', train=True, download=True, \n",
    "                   transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "\n",
    "train_size = int(0.9 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "train_subset, val_subset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "# images, labels = next(iter(train_loader))\n",
    "\n",
    "# plt.imshow(torchvision.utils.make_grid(images).permute(1, 2, 0) / 2 + 0.5)\n",
    "# plt.title(' '.join(trainset.classes[label] for label in labels))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da8b0722",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e857e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarDNN(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Flatten())\n",
    "        self.layers.append(nn.Linear(n_inputs, n_hidden, device=device))\n",
    "        self.layers.append(nn.SiLU())\n",
    "        for i in range(20):\n",
    "            self.layers.append(nn.Linear(n_hidden, n_hidden, device=device))\n",
    "            self.layers.append(nn.SiLU())\n",
    "        self.layers.append(nn.Linear(n_hidden, n_outputs))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layers in self.layers:\n",
    "            x = layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "223a7f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_acc = 0\n",
    "    \n",
    "    def step(self, val_acc):\n",
    "        if val_acc > self.best_acc:\n",
    "            self.best_acc = val_acc\n",
    "            self.counter = 0\n",
    "            return False  # do not stop\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            return self.counter >= self.patience  # stop if patience exhausted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa2383ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CifarDNN(\n",
       "  (layers): ModuleList(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=3072, out_features=50, bias=True)\n",
       "    (2): SiLU()\n",
       "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (4): SiLU()\n",
       "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (6): SiLU()\n",
       "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (8): SiLU()\n",
       "    (9): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (10): SiLU()\n",
       "    (11): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (12): SiLU()\n",
       "    (13): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (14): SiLU()\n",
       "    (15): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (16): SiLU()\n",
       "    (17): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (18): SiLU()\n",
       "    (19): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (20): SiLU()\n",
       "    (21): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (22): SiLU()\n",
       "    (23): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (24): SiLU()\n",
       "    (25): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (26): SiLU()\n",
       "    (27): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (28): SiLU()\n",
       "    (29): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (30): SiLU()\n",
       "    (31): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (32): SiLU()\n",
       "    (33): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (34): SiLU()\n",
       "    (35): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (36): SiLU()\n",
       "    (37): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (38): SiLU()\n",
       "    (39): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (40): SiLU()\n",
       "    (41): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (42): SiLU()\n",
       "    (43): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights_he(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "model = CifarDNN(3*32*32, 50, 10).to(device)\n",
    "model.apply(init_weights_he)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12ed2ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, target):\n",
    "    preds = pred.argmax(dim=1)\n",
    "    return (preds == target).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e865eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.NAdam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "early_stopper = EarlyStopping(patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34b04b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 1.9253 | Train Acc: 0.2702 | Val Loss: 1.8156 | Val Acc: 0.3212\n",
      "Epoch 02 | Train Loss: 1.7104 | Train Acc: 0.3745 | Val Loss: 1.7320 | Val Acc: 0.3742\n",
      "Epoch 03 | Train Loss: 1.6166 | Train Acc: 0.4120 | Val Loss: 1.6491 | Val Acc: 0.3902\n",
      "Epoch 04 | Train Loss: 1.5462 | Train Acc: 0.4378 | Val Loss: 1.6348 | Val Acc: 0.4108\n",
      "Epoch 05 | Train Loss: 1.4966 | Train Acc: 0.4645 | Val Loss: 1.5998 | Val Acc: 0.4126\n",
      "Epoch 06 | Train Loss: 1.4458 | Train Acc: 0.4818 | Val Loss: 1.5933 | Val Acc: 0.4351\n",
      "Epoch 07 | Train Loss: 1.4029 | Train Acc: 0.5002 | Val Loss: 1.5322 | Val Acc: 0.4563\n",
      "Epoch 08 | Train Loss: 1.3694 | Train Acc: 0.5125 | Val Loss: 1.5619 | Val Acc: 0.4508\n",
      "Epoch 09 | Train Loss: 1.3425 | Train Acc: 0.5248 | Val Loss: 1.5143 | Val Acc: 0.4602\n",
      "Epoch 10 | Train Loss: 1.3169 | Train Acc: 0.5344 | Val Loss: 1.5033 | Val Acc: 0.4763\n",
      "Epoch 11 | Train Loss: 1.2914 | Train Acc: 0.5428 | Val Loss: 1.5209 | Val Acc: 0.4650\n",
      "Epoch 12 | Train Loss: 1.2712 | Train Acc: 0.5525 | Val Loss: 1.5193 | Val Acc: 0.4761\n",
      "Epoch 13 | Train Loss: 1.2477 | Train Acc: 0.5611 | Val Loss: 1.5004 | Val Acc: 0.4711\n",
      "Epoch 14 | Train Loss: 1.2274 | Train Acc: 0.5695 | Val Loss: 1.5439 | Val Acc: 0.4721\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device) \n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy(pred, y_batch)\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            pred = model(X_batch)\n",
    "            loss = criterion(pred, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            val_acc += accuracy(pred, y_batch)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if early_stopper.step(val_acc):\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8056000c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
