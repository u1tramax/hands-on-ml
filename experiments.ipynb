{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8b9fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f71211c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "dataset = load_diabetes(scaled=True)\n",
    "X, y = dataset['data'], dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "109a52e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "full_dataset = DiabetesDataset(X, y)\n",
    "\n",
    "train_size = int(len(X) * 0.8)\n",
    "test_size = len(X) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "710bbfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(y_pred, y_true):\n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2)\n",
    "    return 1 - ss_res / ss_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "489116e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs, num_hidden), nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d94f140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(X.shape[1], 10, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ee7dbbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 58.6141 | R2: -inf | Test Loss: 66.3251 | R2 -0.0004635\n",
      "Epoch 1 | Train Loss: 56.7045 | R2: -inf | Test Loss: 62.4993 | R2 0.1182\n",
      "Epoch 2 | Train Loss: 56.6191 | R2: -inf | Test Loss: 57.5709 | R2 0.2554\n",
      "Epoch 3 | Train Loss: 55.4683 | R2: -inf | Test Loss: 82.0527 | R2 -0.5745\n",
      "Epoch 4 | Train Loss: 62.0880 | R2: -inf | Test Loss: 115.4230 | R2 -2.163\n",
      "Epoch 5 | Train Loss: 66.2551 | R2: -inf | Test Loss: 122.0380 | R2 -2.541\n",
      "Epoch 6 | Train Loss: 64.0685 | R2: -inf | Test Loss: 88.7397 | R2 -0.8495\n",
      "Epoch 7 | Train Loss: 56.4598 | R2: -inf | Test Loss: 57.6098 | R2 0.2566\n",
      "Epoch 8 | Train Loss: 61.8367 | R2: -inf | Test Loss: 84.6800 | R2 -0.6531\n",
      "Epoch 9 | Train Loss: 64.1870 | R2: -inf | Test Loss: 70.6724 | R2 -0.1422\n",
      "Epoch 10 | Train Loss: 61.0728 | R2: -inf | Test Loss: 94.1871 | R2 -1.088\n",
      "Epoch 11 | Train Loss: 62.5397 | R2: -inf | Test Loss: 76.6594 | R2 -0.3358\n",
      "Epoch 12 | Train Loss: 59.4963 | R2: -inf | Test Loss: 52.4962 | R2 0.3912\n",
      "Epoch 13 | Train Loss: 53.3702 | R2: -inf | Test Loss: 53.5372 | R2 0.3619\n",
      "Epoch 14 | Train Loss: 54.1980 | R2: -inf | Test Loss: 70.9703 | R2 -0.1628\n",
      "Epoch 15 | Train Loss: 53.9572 | R2: -inf | Test Loss: 54.6756 | R2 0.3312\n",
      "Epoch 16 | Train Loss: 54.6764 | R2: -inf | Test Loss: 72.9423 | R2 -0.2312\n",
      "Epoch 17 | Train Loss: 53.9218 | R2: -inf | Test Loss: 54.5103 | R2 0.3358\n",
      "Epoch 18 | Train Loss: 52.1235 | R2: -inf | Test Loss: 52.1435 | R2 0.3983\n",
      "Epoch 19 | Train Loss: 54.8174 | R2: -inf | Test Loss: 58.1181 | R2 0.244\n",
      "Epoch 20 | Train Loss: 55.7913 | R2: -inf | Test Loss: 58.2929 | R2 0.2373\n",
      "Epoch 21 | Train Loss: 55.7443 | R2: -inf | Test Loss: 64.0631 | R2 0.06197\n",
      "Epoch 22 | Train Loss: 53.9795 | R2: -inf | Test Loss: 54.8113 | R2 0.3306\n",
      "Epoch 23 | Train Loss: 53.4317 | R2: -inf | Test Loss: 52.0019 | R2 0.4002\n",
      "Epoch 24 | Train Loss: 53.3840 | R2: -inf | Test Loss: 66.6958 | R2 -0.01964\n",
      "Epoch 25 | Train Loss: 55.0196 | R2: -inf | Test Loss: 52.7046 | R2 0.3821\n",
      "Epoch 26 | Train Loss: 52.4075 | R2: -inf | Test Loss: 59.1842 | R2 0.2067\n",
      "Epoch 27 | Train Loss: 57.2359 | R2: -inf | Test Loss: 73.8727 | R2 -0.2483\n",
      "Epoch 28 | Train Loss: 64.3622 | R2: -inf | Test Loss: 104.6399 | R2 -1.584\n",
      "Epoch 29 | Train Loss: 59.1486 | R2: -inf | Test Loss: 68.9824 | R2 -0.08014\n",
      "Epoch 30 | Train Loss: 57.4287 | R2: -inf | Test Loss: 56.3323 | R2 0.2889\n",
      "Epoch 31 | Train Loss: 56.9548 | R2: -inf | Test Loss: 97.7269 | R2 -1.245\n",
      "Epoch 32 | Train Loss: 55.6408 | R2: -inf | Test Loss: 52.8027 | R2 0.3774\n",
      "Epoch 33 | Train Loss: 52.2030 | R2: -inf | Test Loss: 55.7042 | R2 0.3015\n",
      "Epoch 34 | Train Loss: 59.7401 | R2: -inf | Test Loss: 118.5597 | R2 -2.328\n",
      "Epoch 35 | Train Loss: 60.2890 | R2: -inf | Test Loss: 54.4577 | R2 0.3355\n",
      "Epoch 36 | Train Loss: 53.3892 | R2: -inf | Test Loss: 53.3109 | R2 0.3637\n",
      "Epoch 37 | Train Loss: 53.4756 | R2: -inf | Test Loss: 68.2831 | R2 -0.07146\n",
      "Epoch 38 | Train Loss: 54.0060 | R2: -inf | Test Loss: 71.8989 | R2 -0.1927\n",
      "Epoch 39 | Train Loss: 59.1713 | R2: -inf | Test Loss: 74.9935 | R2 -0.2909\n",
      "Epoch 40 | Train Loss: 63.3011 | R2: -inf | Test Loss: 132.5373 | R2 -3.163\n",
      "Epoch 41 | Train Loss: 59.7861 | R2: -inf | Test Loss: 51.4263 | R2 0.4121\n",
      "Epoch 42 | Train Loss: 53.9889 | R2: -inf | Test Loss: 71.9305 | R2 -0.1931\n",
      "Epoch 43 | Train Loss: 54.4414 | R2: -inf | Test Loss: 57.8781 | R2 0.242\n",
      "Epoch 44 | Train Loss: 53.2965 | R2: -inf | Test Loss: 52.4370 | R2 0.3867\n",
      "Epoch 45 | Train Loss: 52.7228 | R2: -inf | Test Loss: 51.2757 | R2 0.4156\n",
      "Epoch 46 | Train Loss: 54.1312 | R2: -inf | Test Loss: 69.3648 | R2 -0.1064\n",
      "Epoch 47 | Train Loss: 52.2896 | R2: -inf | Test Loss: 55.0856 | R2 0.3183\n",
      "Epoch 48 | Train Loss: 51.8948 | R2: -inf | Test Loss: 52.0107 | R2 0.3971\n",
      "Epoch 49 | Train Loss: 55.5578 | R2: -inf | Test Loss: 56.7833 | R2 0.2739\n",
      "Epoch 50 | Train Loss: 54.4843 | R2: -inf | Test Loss: 52.6363 | R2 0.3806\n",
      "Epoch 51 | Train Loss: 53.6083 | R2: -inf | Test Loss: 62.3049 | R2 0.1159\n",
      "Epoch 52 | Train Loss: 54.0859 | R2: -inf | Test Loss: 53.0782 | R2 0.3695\n",
      "Epoch 53 | Train Loss: 54.0439 | R2: -inf | Test Loss: 60.3627 | R2 0.1732\n",
      "Epoch 54 | Train Loss: 52.6097 | R2: -inf | Test Loss: 51.1372 | R2 0.4179\n",
      "Epoch 55 | Train Loss: 52.6737 | R2: -inf | Test Loss: 55.9690 | R2 0.2928\n",
      "Epoch 56 | Train Loss: 55.4534 | R2: -inf | Test Loss: 64.5443 | R2 0.04928\n",
      "Epoch 57 | Train Loss: 56.0524 | R2: -inf | Test Loss: 68.0000 | R2 -0.06111\n",
      "Epoch 58 | Train Loss: 55.3968 | R2: -inf | Test Loss: 64.4910 | R2 0.0527\n",
      "Epoch 59 | Train Loss: 60.7821 | R2: -inf | Test Loss: 80.7111 | R2 -0.5198\n",
      "Epoch 60 | Train Loss: 58.9012 | R2: -inf | Test Loss: 69.9623 | R2 -0.1132\n",
      "Epoch 61 | Train Loss: 58.0007 | R2: -inf | Test Loss: 70.7762 | R2 -0.159\n",
      "Epoch 62 | Train Loss: 55.9733 | R2: -inf | Test Loss: 59.5303 | R2 0.1945\n",
      "Epoch 63 | Train Loss: 53.1564 | R2: -inf | Test Loss: 52.4706 | R2 0.3843\n",
      "Epoch 64 | Train Loss: 52.0781 | R2: -inf | Test Loss: 51.1913 | R2 0.4152\n",
      "Epoch 65 | Train Loss: 57.6484 | R2: -inf | Test Loss: 72.1661 | R2 -0.1919\n",
      "Epoch 66 | Train Loss: 59.9933 | R2: -inf | Test Loss: 76.9200 | R2 -0.3542\n",
      "Epoch 67 | Train Loss: 61.5033 | R2: -inf | Test Loss: 89.9509 | R2 -0.9046\n",
      "Epoch 68 | Train Loss: 57.1751 | R2: -inf | Test Loss: 54.3680 | R2 0.3434\n",
      "Epoch 69 | Train Loss: 53.2895 | R2: -inf | Test Loss: 51.8290 | R2 0.4031\n",
      "Epoch 70 | Train Loss: 55.9687 | R2: -inf | Test Loss: 90.6331 | R2 -0.9277\n",
      "Epoch 71 | Train Loss: 56.5651 | R2: -inf | Test Loss: 58.7241 | R2 0.2178\n",
      "Epoch 72 | Train Loss: 53.4348 | R2: -inf | Test Loss: 69.6709 | R2 -0.1198\n",
      "Epoch 73 | Train Loss: 54.5596 | R2: -inf | Test Loss: 57.4146 | R2 0.2568\n",
      "Epoch 74 | Train Loss: 53.6280 | R2: -inf | Test Loss: 53.2129 | R2 0.3667\n",
      "Epoch 75 | Train Loss: 54.9275 | R2: -inf | Test Loss: 58.6272 | R2 0.2233\n",
      "Epoch 76 | Train Loss: 55.0538 | R2: -inf | Test Loss: 52.1177 | R2 0.3953\n",
      "Epoch 77 | Train Loss: 59.4187 | R2: -inf | Test Loss: 86.3565 | R2 -0.7395\n",
      "Epoch 78 | Train Loss: 60.9655 | R2: -inf | Test Loss: 89.7180 | R2 -0.8865\n",
      "Epoch 79 | Train Loss: 59.3145 | R2: -inf | Test Loss: 96.5756 | R2 -1.195\n",
      "Epoch 80 | Train Loss: 56.9094 | R2: -inf | Test Loss: 57.8645 | R2 0.241\n",
      "Epoch 81 | Train Loss: 54.5766 | R2: -inf | Test Loss: 55.4414 | R2 0.3105\n",
      "Epoch 82 | Train Loss: 52.8589 | R2: -inf | Test Loss: 51.5346 | R2 0.409\n",
      "Epoch 83 | Train Loss: 52.5986 | R2: -inf | Test Loss: 51.4848 | R2 0.4103\n",
      "Epoch 84 | Train Loss: 64.1556 | R2: -inf | Test Loss: 105.5443 | R2 -1.62\n",
      "Epoch 85 | Train Loss: 63.1008 | R2: -inf | Test Loss: 53.7888 | R2 0.3513\n",
      "Epoch 86 | Train Loss: 52.7412 | R2: -inf | Test Loss: 56.8741 | R2 0.2689\n",
      "Epoch 87 | Train Loss: 56.9432 | R2: -inf | Test Loss: 91.0126 | R2 -0.9429\n",
      "Epoch 88 | Train Loss: 56.6976 | R2: -inf | Test Loss: 55.3678 | R2 0.3121\n",
      "Epoch 89 | Train Loss: 53.2026 | R2: -inf | Test Loss: 51.5958 | R2 0.4073\n",
      "Epoch 90 | Train Loss: 54.0717 | R2: -inf | Test Loss: 71.6437 | R2 -0.1868\n",
      "Epoch 91 | Train Loss: 55.9236 | R2: -inf | Test Loss: 55.2835 | R2 0.3139\n",
      "Epoch 92 | Train Loss: 54.8934 | R2: -inf | Test Loss: 60.2223 | R2 0.1781\n",
      "Epoch 93 | Train Loss: 55.3820 | R2: -inf | Test Loss: 68.8454 | R2 -0.09244\n",
      "Epoch 94 | Train Loss: 53.8079 | R2: -inf | Test Loss: 56.0198 | R2 0.2943\n",
      "Epoch 95 | Train Loss: 55.7132 | R2: -inf | Test Loss: 75.4278 | R2 -0.3199\n",
      "Epoch 96 | Train Loss: 53.9913 | R2: -inf | Test Loss: 51.4282 | R2 0.4115\n",
      "Epoch 97 | Train Loss: 54.6735 | R2: -inf | Test Loss: 73.9891 | R2 -0.268\n",
      "Epoch 98 | Train Loss: 54.6651 | R2: -inf | Test Loss: 53.9202 | R2 0.3491\n",
      "Epoch 99 | Train Loss: 53.4254 | R2: -inf | Test Loss: 53.1538 | R2 0.3689\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    train_loss, train_r2 = 0, 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_r2 += r2_score(y_pred, y_batch).item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_r2 /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss, test_r2 = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_r2 += r2_score(y_pred, y_batch).item()\n",
    "        \n",
    "        test_loss /= len(test_loader)\n",
    "        test_r2 /= len(test_loader)\n",
    "\n",
    "    print(f'Epoch {epoch} | Train Loss: {train_loss ** 0.5:.4f} | R2: {train_r2:.4f} | Test Loss: {test_loss ** 0.5:.4f} | R2 {test_r2:.4g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68677406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
