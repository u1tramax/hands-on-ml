{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dacd361c",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "**Task:** *Tune the hyperparameters of the training to beat my best validation loss of 2.2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e66b1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().split()\n",
    "alphabet = sorted(list(set([ch for w in words for ch in w])))\n",
    "stoi = {s: i+1 for i, s in enumerate(alphabet)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92485347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# build the dataset\n",
    "block_size = 4 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):\n",
    "  X, Y = [], []\n",
    "  for w in words:\n",
    "\n",
    "    #print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  #print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed4d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((27, 10), generator=g)\n",
    "W1 = torch.randn((40, 200), generator=g)\n",
    "b1 = torch.randn(200, generator=g)\n",
    "W2 = torch.randn((200, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "lre = torch.linspace(-3, 0, 1000)\n",
    "lrs = 10**lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c030a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri = []\n",
    "lossi = []\n",
    "stepi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60427530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "for i in range(200000):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (32,))\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xtr[ix]] # (32, 3, 2)\n",
    "  h = torch.tanh(emb.view(-1, 40) @ W1 + b1) # (32, 200)\n",
    "  logits = h @ W2 + b2 # (32, 27)\n",
    "  loss = F.cross_entropy(logits, Ytr[ix])\n",
    "  #print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  #lr = lrs[i]\n",
    "  #lr = 0.1 if i < 100000 else 0.01\n",
    "  lr = 0.01\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  #lri.append(lre[i])\n",
    "  stepi.append(i)\n",
    "  lossi.append(loss.log10().item())\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc1d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(stepi, 10**torch.tensor([lossi]).view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56e17a0",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "**Task:** *I was not careful with the intialization of the network in this video. (1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve? (2) Can you tune the initialization to get a starting loss that is much more similar to (1)?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5059b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((27, 10), generator=g)\n",
    "W1 = torch.randn((40, 200), generator=g)\n",
    "b1 = torch.randn(200, generator=g)\n",
    "W2 = torch.randn((200, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "\n",
    "torch.nn.init.xavier_uniform_(C)\n",
    "torch.nn.init.xavier_uniform_(W1)\n",
    "torch.nn.init.xavier_uniform_(W2)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276b4b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri = []\n",
    "lossi = []\n",
    "stepi = []\n",
    "\n",
    "for i in range(200000):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (32,))\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xtr[ix]] # (32, 4, 10)\n",
    "  h = torch.tanh(emb.view(-1, 40) @ W1 + b1) # (32, 200)\n",
    "  logits = h @ W2 + b2 # (32, 27)\n",
    "  loss = F.cross_entropy(logits, Ytr[ix])\n",
    "  #print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  #lr = lrs[i]\n",
    "  lr = 0.1 if i < 100000 else 0.01\n",
    "  #lr = 0.01\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  #lri.append(lre[i])\n",
    "  stepi.append(i)\n",
    "  lossi.append(loss.log10().item())\n",
    "\n",
    "print(loss.item())\n",
    "# I guess it's lower than with default init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f6854",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[Xdev] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, 40) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efa2e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stepi, 10**torch.tensor([lossi]).view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346114b7",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "**Task:** *Read the Bengio et al 2003 paper (link above), implement and try any idea from the paper. Did it work?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa24f5ba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
